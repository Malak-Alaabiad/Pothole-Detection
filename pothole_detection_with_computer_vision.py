# -*- coding: utf-8 -*-
"""Pothole Detection with Computer Vision.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p-1yOnLd-3ummkLkuKUBYlgzAV2jXKf_

### Build a computer vision-based technology to process and detect the potholes present in an image.

### Import necessary packages and libraries
"""

!pip install distance

!pip install imagehash

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import skimage.color
import skimage.util
import imagehash
import cv2
import os
import itertools
import distance
import time
import warnings
from matplotlib import pyplot as plt
from PIL import Image, ImageDraw
# %matplotlib inline
warnings.filterwarnings("ignore")

"""### Mount drive"""

from google.colab import drive
drive.mount('/content/drive')

"""### Load the training dataset"""

train=pd.read_csv("/content/drive/MyDrive/AI model Data/train/train/labels.csv")

"""### Structure of train dataset"""

train.info()

"""The above information shows that there are 1371 annotated images.

### A quick look at the training dataset
"""

train.head()

"""### See the target column distribution"""

sns.countplot(x=train['LabelName']);

train['LabelName'].value_counts()

"""The above plot and summary explain that all images are categorized under a single class (pothole class).

### Each image has one or more than one annotations. Calculate the number of annotations for each image.
"""

train['total_pothole']=train.groupby('ImageID')['ImageID'].transform('count')

sns.countplot(x=train['total_pothole'], order=train['total_pothole'].value_counts().index);

train['total_pothole'].value_counts()

"""The above plot and summary explain that most of the images have 1 to 5 annotation information.

### Extract the basic information about the image like width, height, and color mode.
"""

def basic_image_info(path):
    ImageID=[]
    img_mode=[]
    img_height=[]
    img_width=[]
    for subdir, dirs, files in os.walk(path):
        for file in files:
            ImageID.append(file)
            img=Image.open(f"{(os.path.join(subdir, file))}")
            img_mode.append(img.mode)
            img_width.append(img.width)
            img_height.append(img.height)


    return pd.DataFrame({'ImageID':ImageID,'img_mode':img_mode,'img_width':img_width,'img_height':img_height})

train_image_basic_info=basic_image_info("/content/drive/MyDrive/AI model Data/train/train/images")

"""### See if there are any different color modes in the images."""

train_image_basic_info['img_mode'].value_counts()

"""The above information explains that all images are in RGB color mode.

### See the width distribution of the image
"""

sns.kdeplot(x=train_image_basic_info['img_width']);

sns.boxplot(y=train_image_basic_info['img_width']);

train_image_basic_info['img_width'].describe()

"""#### The above kernel density plot and information explain that the distribution of image width is right skewed.
#### The average image width is 702 pixels.
#### The image width ranges from 194 to 4640 pixels.
#### The boxplot explains that there are images with more than 1000 pixels in width.

### Compare the number of annotations and the width of the image.
"""

train_image_basic_info=pd.merge(train_image_basic_info,train[['ImageID','total_pothole']],on='ImageID',how='left')

"""### Create a function for side by side plot (density and group-wise boxplot)."""

def side_by_side_plot(df,grp,valcol,rot=None,size=(15,6)):
    clr="Paired"
    fig,(ax1,ax2) = plt.subplots(1,2,figsize=size)
    fig.tight_layout()
    sns.kdeplot(x=df[valcol], hue=df[grp],ax=ax1,palette=clr)
    ax1.set_title(grp.capitalize()+" Wise "+valcol.capitalize()+" Distribution",size=15)
    ax1.set_xlabel(valcol,fontsize=20)
    sns.boxplot(x=df[grp],y=df[valcol],ax=ax2,palette=clr)
    ax2.set_title(grp.capitalize()+" Wise "+valcol.capitalize()+" Distribution",size=15)
    ax2.set_xlabel(grp,fontsize=20)
    ax2.set_ylabel("")
    ax2.tick_params(rotation=rot)

side_by_side_plot(train_image_basic_info,'total_pothole','img_width')

train_image_basic_info.groupby('total_pothole')['img_width'].describe()

"""#### The above plot and summary explain that the images that have 1 to 6 annotations are more than 800 pixels in height.

### See the height distribution of the image.
"""

sns.kdeplot(x=train_image_basic_info['img_height']);

sns.boxplot(y=train_image_basic_info['img_height']);

train_image_basic_info['img_height'].describe()

"""#### The above kernel density plot and information explain that the distribution of image height is right skewed.
#### The average image height is 479 pixels.
#### The image height ranges from 150 to 3480 pixels.
#### The boxplot explains that there are images with more than 1000 pixels in height.

### Compare the number of annotations and the width of the image.
"""

side_by_side_plot(train_image_basic_info,'total_pothole','img_height')

train_image_basic_info.groupby('total_pothole')['img_height'].describe()

"""#### The above plot and summary explain that the images that have 1 to 5 annotations are more than 800 pixels in height.

### See the RGB color mode distribution of the image by the number of annotations (pothole).
"""

def rgb_dist_plot(img,ax):
    start=0
    end=256
    for _,color in enumerate(['Red','Green','Blue']):
        _=sns.kdeplot(img.histogram()[start:end],label=color,color=color)
        _=plt.legend();
        start+=256
        end+=256

def image_and_rgb_dist(path,df,group,imgidcol):
    for im in df.groupby([group])[imgidcol].head(1):
        fig, axs = plt.subplots(1, 2 ,figsize=(15,5))
        img = Image.open(f"{(os.path.join(path, im))}")
        axs[0].imshow(img)
        axs[0].axis('off')
        axs[0].set_title(im,fontsize=18)
        _=rgb_dist_plot(img,ax=axs[1])
        axs[1].set_title("RGB Color Distribution For "+im,fontsize=18)

image_and_rgb_dist("/content/drive/MyDrive/AI model Data/train/train/images",train_image_basic_info,'total_pothole','ImageID')

"""#### Single image is taken from each number of pothole categories and analyzed the image's RGB Color distribution.
#### The above RGB distribution explains that blue is the dominant color in all images.

### Calculate area of bounding box(annotated value).
"""

train['bbox_area']=train.apply(lambda df:((df['XMax']-df['XMin'])*(df['YMax']-df['YMin'])), axis=1)

sns.kdeplot(np.log2(train['bbox_area']));

sns.boxplot(y=np.log2(train['bbox_area']))

"""#### For better visualization log transformation is applied.
#### The bounding box area distribution looks normal.
#### Create a cluster for the bbox_area.
"""

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

"""### Create standard scaler and apply to the bbox_area column."""

scl=StandardScaler()

"""### Create a kmeans cluster with a range of 2 to 8 number of cluster and select the best one."""

iner=[]
for k in range(2,8):
    km=KMeans(n_clusters=k).fit(scl.fit_transform(train[['bbox_area']]))
    iner.append(km.inertia_)

sns.lineplot(x=range(2,8),y=iner);

"""#### From the above elbow plot the n_cluster 3 is best one. Let's create a kmeans with 3 clusters."""

km=KMeans(n_clusters=3).fit(scl.fit_transform(train[['bbox_area']]))

"""### Extract the cluster labels."""

train['bbarclus']=km.labels_

"""### Visualize sample images from each clusters."""

def img_read(path,im ,new_size=False):
    img = cv2.imread(f"{(os.path.join(path, im))}")
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    if new_size:
        img=cv2.resize(img,(new_size,new_size))
        return img
    else:
        return img

def bbbox_annotate(img,bbox_df,new_size=False):
    if new_size:
        y_ = img.shape[0]
        x_ = img.shape[1]
        x_scale = new_size / x_
        y_scale = new_size / y_
        img=cv2.resize(img,(new_size,new_size))
        for index, row in bbox_df.iterrows():
            imgs=(cv2.rectangle(img,(int(np.round(row['XMin']*x_scale)),int(np.round(row['YMin']*y_scale))),
                                          (int(np.round(row["XMax"]*x_scale)),int(np.round(row["YMax"]*y_scale))),(0,255,0),5))
        return imgs
    else:
        for index, row in bbox_df.iterrows():
            imgs=(cv2.rectangle(img,(row['XMin'],row['YMin']),
                                          (row["XMax"],row["YMax"]),(0,255,0),5))
        return imgs

path='/content/drive/MyDrive/AI model Data/train/train/images'
fig, axs = plt.subplots(3, 1 ,figsize=(8, 10))
i=1
for im,ax in zip(train.groupby(['bbarclus'])['ImageID'].head(1),axs.flatten()):
    img=img_read(path,im)
    ax.imshow(bbbox_annotate(img,train[train['ImageID']==im].iloc[:,2:6]))
    ax.axis('off')
    ax.set_title(f"Cluster {i}")
    i+=1

"""#### The above images explain that the non-bounding box area region is gradually decreased in every cluster.
#### In cluster 3 the pothole images are ultra-zoomed or captured only the area.

### See sample images by total pothole category.
"""

path='/content/drive/MyDrive/AI model Data/train/train/images'
fig, axs = plt.subplots(7, 2 ,figsize=(8, 10))
for i,(im,ax) in enumerate(zip(train.groupby(['total_pothole'])['ImageID'].head(1),axs.flatten())):
    img=img_read(path,im)
    ax.imshow(bbbox_annotate(img,train[train['ImageID']==im].iloc[:,2:6]))
    ax.axis('off')
    ax.set_title(f"Image{i+1}")

"""#### The images are in different pixels so let's resize all images in same pixels."""

path='/content/drive/MyDrive/AI model Data/train/train/images'
fig, axs = plt.subplots(7, 2 ,figsize=(8, 30))
for i,(im,ax) in enumerate(zip(train.groupby(['total_pothole'])['ImageID'].head(1),axs.flatten())):
    img=img_read(path,im)
    ax.imshow(bbbox_annotate(img,train[train['ImageID']==im].iloc[:,2:6],new_size=800))
    ax.axis('off')
    ax.set_title(f"Image")

"""#### The above images explain that there are images with overlapping bounding boxes.

### Calculate the hash value for the image by using various image hashing algorithms.
"""

def get_image_hash(path):
    ImageID=[]
    img_avg_hash=[]
    img_phash=[]
    img_dhash=[]
    img_whash=[]
    img_color_hash=[]
    for subdir, dirs, files in os.walk(path):
        for file in files:
            ImageID.append(file)
            img=Image.open(f"{(os.path.join(subdir, file))}")
            img_avg_hash.append(imagehash.average_hash(img))
            img_phash.append(imagehash.phash(img))
            img_dhash.append(imagehash.dhash(img))
            img_whash.append(imagehash.whash(img))
            img_color_hash.append(imagehash.colorhash(img))


    return pd.DataFrame({'ImageID':ImageID,'img_avg_hash':img_avg_hash,'img_phash':img_phash,'img_dhash':img_dhash,
                             'img_whash':img_whash,'img_color_hash':img_color_hash})

train_hash_info=get_image_hash("/content/drive/MyDrive/AI model Data/train/train/images")

def image_simliarity(path,df,hash_type,hash_name ,return_df=False):
    img_and_hash=[(i,j) for i,j in zip(list(df['ImageID']),list(df[hash_type].astype('str')))]
    ref_image=[]
    similar_img=[]
    for img1, img2 in itertools.combinations(img_and_hash,2):
        f1, hash1 = img1
        f2, hash2 = img2
        hashdif = distance.hamming(hash1, hash2)
        if hashdif < 5:
        #print(f"images are similar due to dhash", "image1", f1, "image2", f2)
            ref_image.append(f1)
            similar_img.append(f2)
    if return_df:
        return pd.DataFrame({'reference_image':ref_image,'similar_image':similar_img})
    else:
        print(f"Total Similar Images Based on {hash_name} Algorithm {len(ref_image)}")
        for image1,image2 in zip(ref_image[:10],similar_img[:10]):
            fig, axs = plt.subplots(1, 2 ,figsize=(10, 8))
            img1=img_read(path,image1)
            axs[0].imshow(bbbox_annotate(img1,train[train['ImageID']==image1].iloc[:,2:6],new_size=800))
            axs[0].axis('off')
            axs[0].set_title(f"Reference Image {image1}")
            img2=img_read(path,image2)
            axs[1].imshow(bbbox_annotate(img2,train[train['ImageID']==image2].iloc[:,2:6],new_size=800))
            axs[1].axis('off')
            axs[1].set_title(f"Similar Image {image2}")
        plt.show()

"""### Average Hashing

"""

start = time.time()
display(image_simliarity("/content/drive/MyDrive/AI model Data/train/train/images",train_hash_info,hash_type='img_avg_hash',hash_name="Average Hashing",return_df=True))
image_simliarity("/content/drive/MyDrive/AI model Data/train/train/images",train_hash_info,hash_type='img_avg_hash',hash_name="Average Hashing")
end = time.time()
print(end - start)

"""### Perceptual Hashing"""

start = time.time()
display(image_simliarity("/content/drive/MyDrive/AI model Data/train/train/images",
                 train_hash_info,hash_type='img_phash',hash_name="Perceptual Hashing",return_df=True))
image_simliarity("/content/drive/MyDrive/AI model Data/train/train/images",
                 train_hash_info,hash_type='img_phash',hash_name="Perceptual Hashing")
end = time.time()
print(end - start)

"""### Difference Hash"""

start = time.time()
display(image_simliarity("/content/drive/MyDrive/AI model Data/train/train/images",
                 train_hash_info,hash_type='img_phash',hash_name="Difference Hash",return_df=True))
image_simliarity("/content/drive/MyDrive/AI model Data/train/train/images",
                 train_hash_info,hash_type='img_phash',hash_name="Difference Hash")
end = time.time()
print(end - start)

"""### Wavelet Hashing"""

start = time.time()
display(image_simliarity("/content/drive/MyDrive/AI model Data/train/train/images",
                 train_hash_info,hash_type='img_whash',hash_name="Wavelet Hashing",return_df=True))
image_simliarity("/content/drive/MyDrive/AI model Data/train/train/images",
                 train_hash_info,hash_type='img_whash',hash_name="Wavelet Hashing")
end = time.time()
print(end - start)

"""### Color Hashing"""

start = time.time()
display(image_simliarity("/content/drive/MyDrive/AI model Data/train/train/images",
                 train_hash_info,hash_type='img_color_hash',hash_name="Color Hashing",return_df=True))
image_simliarity("/content/drive/MyDrive/AI model Data/train/train/images",
                 train_hash_info,hash_type='img_color_hash',hash_name="Color Hashing")
end = time.time()
print(end - start)

"""### From the above various image hashing algorithm, the perceptual hashing, and difference hashing algorithms significantly find similar images based on the hash value."""

def image_simliarity_test_data(train_path,test_path,train_df,test_df,hash_name ,return_df=False):
    #train_img_and_hash=[(i,j) for i,j in zip(list(train_df['ImageID']),list(train_df[hash_type].astype('str')))]
    #test_img_and_hash=[(i,j) for i,j in zip(list(test_df['ImageID']),list(test_df[hash_type].astype('str')))]

    train_test_img_name=[(x,y) for x in train_df['ImageID'][500:600] for y in test_df['ImageID'][:100]]

    ref_image=[]
    similar_img=[]
    for img1, img2 in train_test_img_name:
        f1 = img1
        hash1=train_df[train_df['ImageID']==img1]['img_avg_hash'].values
        f2 = img2
        hash2=test_df[test_df['ImageID']==img2]['img_avg_hash'].values
        hashdif = distance.hamming(hash1, hash2)
        if hashdif < 5:
        #print(f"images are similar due to dhash", "image1", f1, "image2", f2)
            ref_image.append(f1)
            similar_img.append(f2)
    if return_df:
        return pd.DataFrame({'reference_image':ref_image,'similar_image':similar_img})
    else:
        print(f"Total Similar Images Based on {hash_name} Algorithm {len(ref_image)}")
        for image1,image2 in zip(ref_image[:10],similar_img[:10]):
            fig, axs = plt.subplots(1, 2 ,figsize=(10, 8))
            img1=img_read(train_path,image1)
            axs[0].imshow(bbbox_annotate(img1,train[train['ImageID']==image1].iloc[:,2:6],new_size=800))
            axs[0].axis('off')
            axs[0].set_title(f"Train Reference Image {image1}")
            img2=img_read(test_path,image2)
            axs[1].imshow(img2)
            axs[1].axis('off')
            axs[1].set_title(f"Test Similar Image {image2}")
        plt.show()